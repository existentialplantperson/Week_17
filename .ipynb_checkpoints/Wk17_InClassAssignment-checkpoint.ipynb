{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a84b5bc1",
   "metadata": {},
   "source": [
    "### Week 17 In-Class Assignment\n",
    "Angela Spencer January 19, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a0641",
   "metadata": {},
   "source": [
    "1. Write simple (straightforward) definitions for the following parameters for RandomForestClassifier \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html and indicate how they correlate with the precision and recall for the basic diabetes model we built in class. You will need to rerun the model multiple times to do so.\n",
    "\n",
    "Parameter- Correlation with Precision - Correlation with Recall\n",
    "- estimators\n",
    "    - precision - positive correlation\n",
    "    - recall - positive correlation\n",
    "    - description - the number of trees to build before aggregating the data together, more estimators means a more accurate representation of the data and so precision and recall are increased\n",
    "- max_depth\n",
    "    - precision - positive\n",
    "    - recall - positive\n",
    "    - decision - this parameter controls the depth of the tree, the number of levels of splitting; increasing the depth of a tree can improve precision and recall because the data is being distilled into more essential components\n",
    "- min_samples_split\n",
    "    - precision - negative\n",
    "    - recall - negative\n",
    "    - description - the number of samples required to split into a new node. The default and the minimum is 2. Increasing this value results in less splitting of the data and goruping with larger amounts remaining at the leaf nodes which would decrease precision and recall as the results are less representative of the population\n",
    "- min_samples_leaf\n",
    "    - precision - negative\n",
    "    - recall - negative\n",
    "    - description - similar to min_samples_split; default and minimum is 1; a node will only be split if it produces a minimum number of samples in the final leaf node (rather than a minimum for each node); results can smooth the regression line by distilling the data down to a certain point and then leaving larger numbers of samples in the final nodes\n",
    "- min_weight_fraction_leaf\n",
    "    - precision - positive\n",
    "    - recall - negative\n",
    "    - description - default is 0, very small changes affected the results\n",
    "- max_leaf_nodes\n",
    "    - precision - positive\n",
    "    - recall - positive\n",
    "    - description - controls the maximum number of final leaf nodes, increasing the number of nodes increases the precision and recall to the point of overfitting \n",
    "- min_impurity_decrease - \"A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\"\n",
    "    - precision - negative correlation\n",
    "    - recall - negative correlation\n",
    "    - description - default is 0, a threshold for splitting a node based on the data is in the parent node; larger values create a large threshold for splitting and decrease precision and recall by preventing data points from being accurately categorized; this affects the minority class the most\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773b0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e44f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('../Datasets/diabetes.csv')\n",
    "\n",
    "# seperate target variable and features\n",
    "X = diabetes.drop('Outcome', axis=1)\n",
    "y = diabetes['Outcome']\n",
    "\n",
    "#split data into training and validation sets using resampled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=66)\n",
    "\n",
    "#define standardizing object\n",
    "scale = StandardScaler()\n",
    "X_train = scale.fit_transform(X_train)\n",
    "X_test = scale.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7502661c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       160\n",
      "           1       0.60      0.69      0.64        71\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.72      0.74      0.73       231\n",
      "weighted avg       0.77      0.76      0.77       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=66)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "#rf.score(X_test, y_test)\n",
    "predictions = rf.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29342a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.79      0.82       160\n",
      "           1       0.60      0.70      0.65        71\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.73      0.75      0.73       231\n",
      "weighted avg       0.78      0.76      0.77       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(bootstrap=False, random_state=66)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "#rf.score(X_test, y_test)\n",
    "predictions = rf.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d8d62c",
   "metadata": {},
   "source": [
    "2. How does setting bootstrap=False influence the model performance? Note: the default is bootstrap=True. Explain why your results might be so.\n",
    "\n",
    "Bootstrapping means pulling random samples of the data many times to build an aggregate of the averages of the data rather than running the classifier on all of the data. It did slightly affect the precision and recall for the minority class. Bootstrapping may have helped identify the traits leading to developing diabetes slightly better than without bootstrapping due to the nature of taking many samples over and over the data. This cna help identify trends in data that is not normally distributed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
